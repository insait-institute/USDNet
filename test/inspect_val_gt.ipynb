{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95f8e393-e589-4b6e-bf5b-8f4eae515356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"/workspace/Mask3D_adapted\")\n",
    "import re, os, json\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import open3d as o3d\n",
    "from fire import Fire\n",
    "from natsort import natsorted\n",
    "from loguru import logger\n",
    "\n",
    "from datasets.preprocessing.base_preprocessing import BasePreprocessing\n",
    "from utils.point_cloud_utils import load_ply_with_normals\n",
    "\n",
    "from datasets.scannetpp.scannetpp_constants import (\n",
    "    CLASS_IDS, VALID_CLASS_IDS, CLASS_LABELS, INSTANCE_LABELS, LABEL2ID)\n",
    "\n",
    "splits = {'train': \"nvs_sem_train.txt\",\n",
    "          'val': \"nvs_sem_val.txt\",\n",
    "          'test': \"sem_test.txt\"}\n",
    "def filter_map_classes(mapping, count_thresh, count_type, mapping_type):\n",
    "    mapping = mapping[mapping[count_type] >= count_thresh]\n",
    "    if mapping_type == \"semantic\":\n",
    "        map_key = \"semantic_map_to\"\n",
    "    elif mapping_type == \"instance\":\n",
    "        map_key = \"instance_map_to\"\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    # create a dict with classes to be mapped\n",
    "    # classes that don't have mapping are entered as x->x\n",
    "    # otherwise x->y\n",
    "    map_dict = OrderedDict()\n",
    "\n",
    "    for i in range(mapping.shape[0]):\n",
    "        row = mapping.iloc[i]\n",
    "        class_name = row[\"class\"]\n",
    "        map_target = row[map_key]\n",
    "\n",
    "        # map to None or some other label -> don't add this class to the label list\n",
    "        try:\n",
    "            if len(map_target) > 0:\n",
    "                # map to None -> don't use this class\n",
    "                if map_target == \"None\":\n",
    "                    pass\n",
    "                else:\n",
    "                    # map to something else -> use this class\n",
    "                    map_dict[class_name] = map_target\n",
    "        except TypeError:\n",
    "            # nan values -> no mapping, keep label as is\n",
    "            if class_name not in map_dict:\n",
    "                map_dict[class_name] = class_name\n",
    "\n",
    "    return map_dict\n",
    "def downsample(points, sem_gt, inst_gt, voxel_size=0.025):\n",
    "    coords = points[:, :3]\n",
    "    colors = points[:, 3:6]\n",
    "    normals = points[:, 6:9]\n",
    "    total_downsample_points = 0\n",
    "    points_list = []\n",
    "    colors_list = []\n",
    "    normals_list = []\n",
    "    sem_list = []\n",
    "    inst_list = []\n",
    "    insts = np.unique(inst_gt)\n",
    "    for inst in insts:\n",
    "        inst_mask = inst_gt == inst\n",
    "        inst_coords = coords[inst_mask]\n",
    "        inst_colors = colors[inst_mask]\n",
    "        inst_normals = normals[inst_mask]\n",
    "        sem_id = sem_gt[inst_mask][0]\n",
    "        \n",
    "        inst_pcd = o3d.geometry.PointCloud()\n",
    "        inst_pcd.points = o3d.utility.Vector3dVector(inst_coords)\n",
    "        inst_pcd.colors = o3d.utility.Vector3dVector(inst_colors)\n",
    "        downinst_pcd = inst_pcd.voxel_down_sample(voxel_size=voxel_size)\n",
    "        \n",
    "        points_list.append(np.asarray(downinst_pcd.points))\n",
    "        colors_list.append(np.asarray(downinst_pcd.colors))\n",
    "        sem_list.append(np.full((len(downinst_pcd.points),), sem_id))\n",
    "        inst_list.append(np.full((len(downinst_pcd.points),), inst))\n",
    "        total_downsample_points += len(downinst_pcd.points)\n",
    "        \n",
    "        # Create an array to hold the downsampled normals\n",
    "        downsampled_inst_normals = []\n",
    "        pcd_tree = o3d.geometry.KDTreeFlann(inst_pcd)\n",
    "        # Iterate over downsampled points\n",
    "        for point in downinst_pcd.points:\n",
    "            # Find the nearest neighbor in the original point cloud\n",
    "            [_, idx, _] = pcd_tree.search_knn_vector_3d(point, 1)\n",
    "            # Get the corresponding normal\n",
    "            downsampled_inst_normals.append(inst_normals[idx[0]])\n",
    "        normals_list.append(np.asarray(downsampled_inst_normals))\n",
    "                \n",
    "    points_dsped = np.concatenate(points_list, axis=0)\n",
    "    colors_dsped = np.concatenate(colors_list, axis=0)\n",
    "    normals_dsped = np.concatenate(normals_list, axis=0)\n",
    "    sem_dsped = np.concatenate(sem_list, axis=0)\n",
    "    inst_dsped = np.concatenate(inst_list, axis=0)\n",
    "    return points_dsped, colors_dsped, normals_dsped, sem_dsped, inst_dsped\n",
    "\n",
    "class ScannetppPreprocessing(BasePreprocessing):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str = \"./data/raw/scannetpp/scannetpp\",\n",
    "        save_dir: str = \"./data/processed/scannetpp\",\n",
    "        modes: tuple = (\"train\", \"val\"),\n",
    "        n_jobs: int = -1,\n",
    "        ignore_index: int = 0\n",
    "    ):\n",
    "        super().__init__(data_dir, save_dir, modes, n_jobs)\n",
    "        # meta data\n",
    "        self.ignore_index = ignore_index\n",
    "        self.create_label_database(data_dir)\n",
    "        label_mapping = pd.read_csv(\n",
    "            osp.join(data_dir, \"metadata\", \"semantic_benchmark\", \"map_benchmark.csv\"))\n",
    "        self.label_mapping = filter_map_classes(\n",
    "            label_mapping, count_thresh=0, count_type=\"count\", mapping_type=\"semantic\"\n",
    "        )\n",
    "        \n",
    "        for mode in self.modes:\n",
    "            split_filename = splits[mode]\n",
    "            split_txt = osp.join(data_dir, \"splits\", split_filename)\n",
    "            with open(split_txt, \"r\") as f:\n",
    "                # read the scan names without the newline character\n",
    "                scans = [line.strip() for line in f]   \n",
    "            folders = []\n",
    "            for scan in scans:\n",
    "                scan_folder = osp.join(data_dir, \"data\", scan)\n",
    "                folders.append(scan_folder)\n",
    "            self.files[mode] = natsorted(folders)\n",
    "\n",
    "    def create_label_database(self, data_dir):\n",
    "        label_database = {}\n",
    "        for row_id, class_id in enumerate(CLASS_IDS):\n",
    "            label_database[class_id] = {\n",
    "                \"name\": CLASS_LABELS[row_id],\n",
    "                \"validation\": class_id in VALID_CLASS_IDS,\n",
    "            }\n",
    "        self._save_yaml(\n",
    "            self.save_dir / \"label_database.yaml\", label_database\n",
    "        )\n",
    "        return label_database\n",
    "\n",
    "    def process_file(self, folderpath, mode):\n",
    "        \"\"\"process_file.\n",
    "\n",
    "        Please note, that for obtaining segmentation labels ply files were used.\n",
    "\n",
    "        Args:\n",
    "            folderpath: path to the scan folder\n",
    "            mode: train, test or validation\n",
    "\n",
    "        Returns:\n",
    "            filebase: info about file\n",
    "        \"\"\"\n",
    "        scan_id = osp.basename(folderpath)\n",
    "        mesh_file = osp.join(folderpath, \"scans/mesh_aligned_0.05.ply\")\n",
    "        segment_file = osp.join(folderpath, \"scans/segments.json\")\n",
    "        anno_file = osp.join(folderpath, \"scans/segments_anno.json\")\n",
    "        filebase = {\n",
    "            \"filepath\": folderpath,\n",
    "            'raw_filepath': str(folderpath),\n",
    "            \"scene\": scan_id,\n",
    "            \"mesh_file\": mesh_file,\n",
    "            'segment_file': segment_file,\n",
    "            'anno_file': anno_file,\n",
    "        }\n",
    "        # reading both files and checking that they are fitting\n",
    "        coords, features, _ = load_ply_with_normals(mesh_file)\n",
    "        file_len = len(coords)\n",
    "        filebase[\"file_len\"] = file_len\n",
    "        points = np.hstack((coords, features))\n",
    "    \n",
    "        # get segment ids and instance ids\n",
    "        with open(segment_file) as f:\n",
    "            segments = json.load(f)\n",
    "        # load anno = (instance, groups of segments)\n",
    "        with open(anno_file) as f:\n",
    "            anno = json.load(f)\n",
    "            \n",
    "        seg_indices = np.array(segments[\"segIndices\"], dtype=np.uint32)\n",
    "        num_vertices = len(seg_indices)\n",
    "        assert num_vertices == points.shape[0]\n",
    "        semantic_gt = np.ones(num_vertices, dtype=np.int32) * self.ignore_index\n",
    "        instance_gt = np.ones(num_vertices, dtype=np.int32) * self.ignore_index\n",
    "        assigned = np.zeros(num_vertices, dtype=bool)\n",
    "        for idx, instance in enumerate(anno[\"segGroups\"]):\n",
    "            label = instance[\"label\"]\n",
    "            # remap label\n",
    "            instance[\"label\"] = self.label_mapping.get(label, None)\n",
    "            instance[\"label_index\"] = LABEL2ID.get(instance[\"label\"], self.ignore_index)\n",
    "            if instance[\"label_index\"] == self.ignore_index:\n",
    "                continue\n",
    "            # get all the vertices with segment index in this instance\n",
    "            # and max number of labels not yet applied\n",
    "            # mask = np.isin(seg_indices, instance[\"segments\"]) & (labels_used < 3)\n",
    "            mask = np.zeros(num_vertices, dtype=bool)\n",
    "            mask[instance[\"segments\"]] = True\n",
    "            mask = np.logical_and(mask, ~assigned)\n",
    "            size = mask.sum()\n",
    "            if size == 0:\n",
    "                continue\n",
    "            # get semantic labels\n",
    "            semantic_gt[mask] = instance[\"label_index\"]\n",
    "            assigned[mask] = True\n",
    "            \n",
    "            # store all valid instance (include ignored instance)\n",
    "            if instance[\"label\"] in INSTANCE_LABELS:\n",
    "                instance_gt[mask] = instance[\"objectId\"]\n",
    "                \n",
    "            gt_label_inspect = instance[\"label_index\"] * 1000 + instance[\"objectId\"] + 1\n",
    "            if gt_label_inspect < 0:\n",
    "                print(\"     instance[label_index]: {}; instance[objectId]: {}\".format([\"label_index\"], instance[\"objectId\"]))\n",
    "                \n",
    "        # downsample the points\n",
    "        coords, colors, normals, sem_gt, inst_gt = downsample(\n",
    "            points, semantic_gt, instance_gt)\n",
    "        segments_placeholder = np.zeros_like(sem_gt)\n",
    "        points = np.hstack((coords, colors, normals, sem_gt[..., None], inst_gt[..., None],\n",
    "                            segments_placeholder[..., None]))\n",
    "        ## save the downsampled points\n",
    "        processed_filepath = (\n",
    "            self.save_dir / mode / f\"{scan_id}.npy\"\n",
    "        )\n",
    "        if not processed_filepath.parent.exists():\n",
    "            processed_filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        np.save(processed_filepath, points.astype(np.float32))\n",
    "        filebase[\"filepath\"] = str(processed_filepath)\n",
    "        \n",
    "        gt_labels = sem_gt * 1000 + inst_gt + 1\n",
    "        processed_gt_filepath = (\n",
    "            self.save_dir\n",
    "            / \"instance_gt\"\n",
    "            / mode\n",
    "            / (scan_id + \".txt\")\n",
    "        )\n",
    "        if not processed_gt_filepath.parent.exists():\n",
    "            processed_gt_filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        np.savetxt(processed_gt_filepath, gt_labels.astype(np.int32), fmt=\"%d\")\n",
    "        filebase[\"instance_gt_filepath\"] = str(processed_gt_filepath)\n",
    "\n",
    "        filebase[\"color_mean\"] = [\n",
    "            float((colors[:, 0] / 255).mean()),\n",
    "            float((colors[:, 1] / 255).mean()),\n",
    "            float((colors[:, 2] / 255).mean()),\n",
    "        ]\n",
    "        filebase[\"color_std\"] = [\n",
    "            float(((colors[:, 0] / 255) ** 2).mean()),\n",
    "            float(((colors[:, 1] / 255) ** 2).mean()),\n",
    "            float(((colors[:, 2] / 255) ** 2).mean()),\n",
    "        ]\n",
    "        return filebase\n",
    "\n",
    "    def compute_color_mean_std(\n",
    "        self,\n",
    "        train_database_path: str = \"./data/processed/scannetpp/train_database.yaml\",\n",
    "    ):\n",
    "        train_database = self._load_yaml(train_database_path)\n",
    "        color_mean, color_std = [], []\n",
    "        for sample in train_database:\n",
    "            color_std.append(sample[\"color_std\"])\n",
    "            color_mean.append(sample[\"color_mean\"])\n",
    "\n",
    "        color_mean = np.array(color_mean).mean(axis=0)\n",
    "        color_std = np.sqrt(np.array(color_std).mean(axis=0) - color_mean**2)\n",
    "        feats_mean_std = {\n",
    "            \"mean\": [float(each) for each in color_mean],\n",
    "            \"std\": [float(each) for each in color_std],\n",
    "        }\n",
    "        self._save_yaml(self.save_dir / \"color_mean_std.yaml\", feats_mean_std)\n",
    "\n",
    "    @logger.catch\n",
    "    def fix_bugs_in_labels(self):\n",
    "        pass\n",
    "    \n",
    "    def joint_database(self, train_modes=[\"train\", \"val\"]):\n",
    "        joint_db = []\n",
    "        for mode in train_modes:\n",
    "            joint_db.extend(\n",
    "                self._load_yaml(self.save_dir / (mode + \"_database.yaml\"))\n",
    "            )\n",
    "        self._save_yaml(\n",
    "            self.save_dir / \"train_validation_database.yaml\", joint_db\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae375528-1a52-4504-a089-c44753bdbcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=\"/workspace/Mask3D_adapted/data/raw/scannetpp\" \n",
    "save_dir=\"/workspace/Mask3D_adapted/data/processed/scannetpp\" \n",
    "\n",
    "preprocessor = ScannetppPreprocessing(data_dir, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "049cb104-6c43-4bc1-b376-877682557a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filepath': '/workspace/Mask3D_adapted/data/processed/scannetpp/val/0d2ee665be.npy',\n",
       " 'raw_filepath': '/workspace/Mask3D_adapted/data/raw/scannetpp/data/0d2ee665be',\n",
       " 'scene': '0d2ee665be',\n",
       " 'mesh_file': '/workspace/Mask3D_adapted/data/raw/scannetpp/data/0d2ee665be/scans/mesh_aligned_0.05.ply',\n",
       " 'segment_file': '/workspace/Mask3D_adapted/data/raw/scannetpp/data/0d2ee665be/scans/segments.json',\n",
       " 'anno_file': '/workspace/Mask3D_adapted/data/raw/scannetpp/data/0d2ee665be/scans/segments_anno.json',\n",
       " 'file_len': 1082845,\n",
       " 'instance_gt_filepath': '/workspace/Mask3D_adapted/data/processed/scannetpp/instance_gt/val/0d2ee665be.txt',\n",
       " 'color_mean': [0.5325205791511989, 0.5141894949042368, 0.4850900186942797],\n",
       " 'color_std': [0.3185788923604686, 0.30037131115908294, 0.27732087317186194]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = \"/workspace/Mask3D_adapted/data/raw/scannetpp/data/0d2ee665be\"\n",
    "\n",
    "preprocessor.process_file(folder_path, \"val\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d9675a3-d053-4a24-a531-4759cee95dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np\n",
    "gt_folder = \"/workspace/Mask3D_adapted/data/processed/scannetpp/instance_gt/validation\"\n",
    "files = os.listdir(gt_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb92db27-1717-4fde-897a-feec1995b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_list = []\n",
    "for file in files:\n",
    "    sem = np.loadtxt(os.path.join(gt_folder, file))\n",
    "    sem = (sem/1000).astype(np.int32)\n",
    "    sem_unqiue = np.unique(sem)\n",
    "    sem_list.extend(sem_unqiue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37028781-19b4-430f-ac8f-86ceb3d1c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_arr = np.unique(np.array(sem_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a03f9270-e9fe-4632-b51b-b8e459548814",
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_arr_valid = sem_arr[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbe4c6da-6a46-4adc-b719-783ccb06b947",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_LABELS = ('table', 'door',\n",
    "'ceiling lamp', 'cabinet', 'blinds', 'curtain', 'chair', 'storage cabinet', 'office chair', 'bookshelf', \n",
    "'whiteboard', 'window', 'box', 'monitor', 'shelf', 'heater', 'kitchen cabinet', 'sofa', 'bed', 'trash can', 'book',\n",
    "'plant', 'blanket', 'tv', 'computer tower', 'refrigerator', 'jacket', 'sink', 'bag', 'picture', 'pillow', 'towel',\n",
    "'suitcase', 'backpack', 'crate', 'keyboard', 'rack', 'toilet', 'printer', 'poster', 'painting', 'microwave', 'shoes',\n",
    "'socket', 'bottle', 'bucket', 'cushion', 'basket', 'shoe rack', 'telephone', 'file folder', 'laptop', 'plant pot',\n",
    "'exhaust fan', 'cup', 'coat hanger', 'light switch', 'speaker', 'table lamp', 'kettle', 'smoke detector', 'container',\n",
    "'power strip', 'slippers', 'paper bag', 'mouse', 'cutting board', 'toilet paper', 'paper towel', 'pot', 'clock',\n",
    "'pan', 'tap', 'jar', 'soap dispenser', 'binder', 'bowl', 'tissue box', 'whiteboard eraser', 'toilet brush', \n",
    "'spray bottle', 'headphones', 'stapler', 'marker'\n",
    ")\n",
    "VALID_CLASS_IDS= np.array([17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,\n",
    "33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, \n",
    "59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, \n",
    "85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100])\n",
    "\n",
    "ID_TO_LABEL = {}\n",
    "LABEL_TO_ID = {}\n",
    "for i in range(len(VALID_CLASS_IDS)):\n",
    "    LABEL_TO_ID[CLASS_LABELS[i]] = VALID_CLASS_IDS[i]\n",
    "    ID_TO_LABEL[VALID_CLASS_IDS[i]] = CLASS_LABELS[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "801dc733-595c-42ba-bda0-d1f9e3f0c69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rack\n",
      "file folder\n",
      "stapler\n"
     ]
    }
   ],
   "source": [
    "for class_id in VALID_CLASS_IDS:\n",
    "    if class_id not in sem_arr_valid:\n",
    "        print(CLASS_LABELS[class_id-17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "404ed2ac-f53f-487d-9373-c06529adad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_check_list = [\"marker\", \"whiteboard eraser\", \"jar\", \"smoke detector\", \"mouse\"]\n",
    "label_check_size = {}\n",
    "data = {}\n",
    "for file in files:\n",
    "    inst = np.loadtxt(os.path.join(gt_folder, file))\n",
    "    sem = (inst/1000).astype(np.int32)\n",
    "    data[file] = {\n",
    "        'inst': inst, 'sem': sem}\n",
    "for label_check in label_check_list:\n",
    "    label_id = LABEL_TO_ID[label_check]\n",
    "    size_list = []\n",
    "\n",
    "    for file in files:\n",
    "        inst = data[file]['inst']\n",
    "        sem = data[file]['sem']\n",
    "        inst_uniques = np.unique(inst[sem == label_id])\n",
    "        for inst_id in inst_uniques:\n",
    "            size_list.append( np.sum(inst == inst_id) )\n",
    "    label_check_size[label_check] = size_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2532553-4d22-492d-b9db-2c9f98d40928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'marker': [17, 16, 54],\n",
       " 'whiteboard eraser': [15, 55, 76, 63, 46, 51, 63],\n",
       " 'jar': [28, 22, 27, 41, 18, 37, 25, 43, 10, 14, 22, 26, 23, 33, 37],\n",
       " 'smoke detector': [62,\n",
       "  79,\n",
       "  92,\n",
       "  52,\n",
       "  51,\n",
       "  82,\n",
       "  85,\n",
       "  37,\n",
       "  73,\n",
       "  19,\n",
       "  9,\n",
       "  79,\n",
       "  75,\n",
       "  56,\n",
       "  33,\n",
       "  55,\n",
       "  69,\n",
       "  51,\n",
       "  66,\n",
       "  35,\n",
       "  41,\n",
       "  52,\n",
       "  45,\n",
       "  19,\n",
       "  47],\n",
       " 'mouse': [13,\n",
       "  24,\n",
       "  38,\n",
       "  20,\n",
       "  25,\n",
       "  24,\n",
       "  43,\n",
       "  32,\n",
       "  26,\n",
       "  36,\n",
       "  25,\n",
       "  41,\n",
       "  30,\n",
       "  38,\n",
       "  34,\n",
       "  26,\n",
       "  39,\n",
       "  29,\n",
       "  16,\n",
       "  53,\n",
       "  26,\n",
       "  39,\n",
       "  32,\n",
       "  35,\n",
       "  34,\n",
       "  40,\n",
       "  25,\n",
       "  41,\n",
       "  37,\n",
       "  29,\n",
       "  27,\n",
       "  23,\n",
       "  23]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_check_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
